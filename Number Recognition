import tensorflow
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense,Flatten
from sklearn.metrics import accuracy_score
(X_train,y_train),(X_test,y_test) = keras.datasets.mnist.load_data()
X_test.shape
(10000, 28, 28)
y_train
array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)
plt.imshow(X_train[4])
<matplotlib.image.AxesImage at 0x24fb9961f50>

X_train = X_train/255
X_test = X_test/255
X_train[0]
array([[0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,
        0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,
        0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.11764706, 0.14117647,
        0.36862745, 0.60392157, 0.66666667, 0.99215686, 0.99215686,
        0.99215686, 0.99215686, 0.99215686, 0.88235294, 0.6745098 ,
        0.99215686, 0.94901961, 0.76470588, 0.25098039, 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.19215686, 0.93333333, 0.99215686,
        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,
        0.99215686, 0.99215686, 0.98431373, 0.36470588, 0.32156863,
        0.32156863, 0.21960784, 0.15294118, 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.07058824, 0.85882353, 0.99215686,
        0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.77647059,
        0.71372549, 0.96862745, 0.94509804, 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.31372549, 0.61176471,
        0.41960784, 0.99215686, 0.99215686, 0.80392157, 0.04313725,
        0.        , 0.16862745, 0.60392157, 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.05490196,
        0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.54509804, 0.99215686, 0.74509804, 0.00784314,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.04313725, 0.74509804, 0.99215686, 0.2745098 ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.1372549 , 0.94509804, 0.88235294,
        0.62745098, 0.42352941, 0.00392157, 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.31764706, 0.94117647,
        0.99215686, 0.99215686, 0.46666667, 0.09803922, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.17647059,
        0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.0627451 , 0.36470588, 0.98823529, 0.99215686, 0.73333333,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.97647059, 0.99215686, 0.97647059,
        0.25098039, 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.18039216,
        0.50980392, 0.71764706, 0.99215686, 0.99215686, 0.81176471,
        0.00784314, 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.15294118, 0.58039216, 0.89803922,
        0.99215686, 0.99215686, 0.99215686, 0.98039216, 0.71372549,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,
        0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.09019608, 0.25882353,
        0.83529412, 0.99215686, 0.99215686, 0.99215686, 0.99215686,
        0.77647059, 0.31764706, 0.00784314, 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.07058824, 0.67058824, 0.85882353, 0.99215686,
        0.99215686, 0.99215686, 0.99215686, 0.76470588, 0.31372549,
        0.03529412, 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.21568627,
        0.6745098 , 0.88627451, 0.99215686, 0.99215686, 0.99215686,
        0.99215686, 0.95686275, 0.52156863, 0.04313725, 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.53333333,
        0.99215686, 0.99215686, 0.99215686, 0.83137255, 0.52941176,
        0.51764706, 0.0627451 , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.        ]])
model = Sequential()

model.add(Flatten(input_shape=(28,28)))
model.add(Dense(128,activation='relu'))
model.add(Dense(32,activation='relu'))
model.add(Dense(10,activation='softmax'))
model.summary()
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten (Flatten)           (None, 784)               0         
                                                                 
 dense (Dense)               (None, 128)               100480    
                                                                 
 dense_1 (Dense)             (None, 32)                4128      
                                                                 
 dense_2 (Dense)             (None, 10)                330       
                                                                 
=================================================================
Total params: 104,938
Trainable params: 104,938
Non-trainable params: 0
_________________________________________________________________
model.compile(loss='sparse_categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])
history = model.fit(X_train,y_train,epochs=25,validation_split=0.2)
Epoch 1/25
1500/1500 [==============================] - 10s 6ms/step - loss: 0.2840 - accuracy: 0.9191 - val_loss: 0.1446 - val_accuracy: 0.9575
Epoch 2/25
1500/1500 [==============================] - 8s 5ms/step - loss: 0.1238 - accuracy: 0.9628 - val_loss: 0.1149 - val_accuracy: 0.9668
Epoch 3/25
1500/1500 [==============================] - 8s 5ms/step - loss: 0.0863 - accuracy: 0.9733 - val_loss: 0.1043 - val_accuracy: 0.9706
Epoch 4/25
1500/1500 [==============================] - 8s 5ms/step - loss: 0.0655 - accuracy: 0.9803 - val_loss: 0.0989 - val_accuracy: 0.9715
Epoch 5/25
1500/1500 [==============================] - 8s 5ms/step - loss: 0.0501 - accuracy: 0.9841 - val_loss: 0.0977 - val_accuracy: 0.9722
Epoch 6/25
1500/1500 [==============================] - 8s 5ms/step - loss: 0.0400 - accuracy: 0.9871 - val_loss: 0.1004 - val_accuracy: 0.9747
Epoch 7/25
1500/1500 [==============================] - 8s 5ms/step - loss: 0.0333 - accuracy: 0.9891 - val_loss: 0.0928 - val_accuracy: 0.9738
Epoch 8/25
1500/1500 [==============================] - 8s 5ms/step - loss: 0.0259 - accuracy: 0.9917 - val_loss: 0.0995 - val_accuracy: 0.9734
Epoch 9/25
1500/1500 [==============================] - 8s 5ms/step - loss: 0.0244 - accuracy: 0.9918 - val_loss: 0.0900 - val_accuracy: 0.9778
Epoch 10/25
1500/1500 [==============================] - 8s 5ms/step - loss: 0.0201 - accuracy: 0.9935 - val_loss: 0.0936 - val_accuracy: 0.9777
Epoch 11/25
1500/1500 [==============================] - 8s 6ms/step - loss: 0.0170 - accuracy: 0.9941 - val_loss: 0.0996 - val_accuracy: 0.9753
Epoch 12/25
1500/1500 [==============================] - 8s 5ms/step - loss: 0.0175 - accuracy: 0.9941 - val_loss: 0.1260 - val_accuracy: 0.9726
Epoch 13/25
1500/1500 [==============================] - 8s 5ms/step - loss: 0.0150 - accuracy: 0.9948 - val_loss: 0.1111 - val_accuracy: 0.9765
Epoch 14/25
1500/1500 [==============================] - 8s 6ms/step - loss: 0.0129 - accuracy: 0.9955 - val_loss: 0.1252 - val_accuracy: 0.9746
Epoch 15/25
1500/1500 [==============================] - 8s 5ms/step - loss: 0.0115 - accuracy: 0.9961 - val_loss: 0.1106 - val_accuracy: 0.9772
Epoch 16/25
1500/1500 [==============================] - 9s 6ms/step - loss: 0.0100 - accuracy: 0.9964 - val_loss: 0.1153 - val_accuracy: 0.9778
Epoch 17/25
1500/1500 [==============================] - 8s 6ms/step - loss: 0.0113 - accuracy: 0.9961 - val_loss: 0.1325 - val_accuracy: 0.9786
Epoch 18/25
1500/1500 [==============================] - 8s 6ms/step - loss: 0.0093 - accuracy: 0.9967 - val_loss: 0.1248 - val_accuracy: 0.9771
Epoch 19/25
1500/1500 [==============================] - 8s 6ms/step - loss: 0.0119 - accuracy: 0.9962 - val_loss: 0.1332 - val_accuracy: 0.9771
Epoch 20/25
1500/1500 [==============================] - 9s 6ms/step - loss: 0.0099 - accuracy: 0.9965 - val_loss: 0.1251 - val_accuracy: 0.9791
Epoch 21/25
1500/1500 [==============================] - 9s 6ms/step - loss: 0.0105 - accuracy: 0.9967 - val_loss: 0.1314 - val_accuracy: 0.9780
Epoch 22/25
1500/1500 [==============================] - 9s 6ms/step - loss: 0.0077 - accuracy: 0.9974 - val_loss: 0.1374 - val_accuracy: 0.9755
Epoch 23/25
1500/1500 [==============================] - 9s 6ms/step - loss: 0.0109 - accuracy: 0.9966 - val_loss: 0.1532 - val_accuracy: 0.9738
Epoch 24/25
1500/1500 [==============================] - 9s 6ms/step - loss: 0.0099 - accuracy: 0.9968 - val_loss: 0.1239 - val_accuracy: 0.9779
Epoch 25/25
1500/1500 [==============================] - 9s 6ms/step - loss: 0.0077 - accuracy: 0.9975 - val_loss: 0.1607 - val_accuracy: 0.9741
y_prob = model.predict(X_test)
313/313 [==============================] - 1s 3ms/step
y_pred = y_prob.argmax(axis=1)
accuracy_score(y_test,y_pred)
0.9724
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
[<matplotlib.lines.Line2D at 0x24fe73445d0>]

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
[<matplotlib.lines.Line2D at 0x24fe740cd50>]

plt.imshow(X_test[4])
<matplotlib.image.AxesImage at 0x24fe4485e90>

model.predict(X_test[4].reshape(1,28,28)).argmax(axis=1)
1/1 [==============================] - 0s 32ms/step
array([4], dtype=int64)
 
